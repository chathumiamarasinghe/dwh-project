---

# â„ï¸ Data Warehouse Project â€” Version 2 (Snowflake)

This version implements the same warehouse architecture using **Snowflake Cloud Data Platform**.

---

## ğŸ”§ Technology Stack

- Snowflake SQL
- Stored Procedures (JavaScript + SQL)
- Snowflake Tasks (Scheduler)
- Internal Stage + File Loading

---

## ğŸ—ï¸ Architecture

Same Medallion pattern:

Bronze â†’ Silver â†’ Gold

yaml


| Layer | Purpose |
|-------|---------|
| Bronze | Raw ingestion from stage |
| Silver | Data cleansing + transformations |
| Gold | Star schema (Dim + Fact tables) |

---

## âš™ï¸ Pipeline Automation

A single **orchestrator task** runs the whole ETL pipeline:

```sql
ALTER TASK orchestrator_task RESUME;
And executes in order:

sql

CALL bronze.load_bronze();
CALL silver.load_silver();

ğŸ“ Folder Structure

snowflake_v2
 â”œâ”€ bronze/
 â”œâ”€ silver/
 â”œâ”€ gold/
 â”œâ”€ stored_procedures/
 â””â”€ tasks/
 â”œâ”€ dags/
 â”‚ â”œâ”€ bronze_layer_load.py
 â”‚ â”œâ”€ silver_layer_load.py
 â”‚ â”œâ”€ gold_layer_load.py
 â”‚ â””â”€ full_etl_pipeline.py
 â””â”€ docker-compose.yml
â–¶ï¸ Running the Pipeline
Create Snowflake database & warehouse.

Run schema setup scripts.

Load sample data into Snowflake stage.

Execute manually:

sql

CALL bronze.load_bronze();
CALL silver.load_silver();
Or enable automation:

sql

ALTER TASK orchestrator_task RESUME;
âœ”ï¸ Status
âœ” Fully operational and scheduled.

```
â–¶ï¸ Running the Pipeline

1ï¸âƒ£ Start Airflow
```sql
 docker compose up -d
```
2ï¸âƒ£ Confirm DAGs are detected
```sql
airflow dags list
```

Expected:

bronze_layer_load
silver_layer_load
gold_layer_load
full_etl_pipeline

3ï¸âƒ£ Trigger Pipeline Manually
```sql
airflow dags trigger full_etl_pipeline
